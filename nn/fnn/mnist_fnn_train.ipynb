{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from mnist_inference import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 构建神经网络需要的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络参数\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVETAGE_DECAY = 0.99\n",
    "\n",
    "# 模型保存路径和文件名\n",
    "MODEL_SAVE_PATH = './logs/'\n",
    "MODEL_NAME = 'mnist_fnn_inference.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 创建训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None,INPUT_NODE],name='x-input')\n",
    "    y_= tf.placeholder(tf.float32, [None,OUTPUT_NODE],name='y-input')\n",
    "    \n",
    "    regularizer=tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    y=inference(x,regularizer)\n",
    "    \n",
    "    # 经过softmax处理\n",
    "#     ys = tf.nn.softmax(y)\n",
    "    \n",
    "    # 挑选概率最大的数字\n",
    "#     y_h = tf.argmax(ys,axis=1)\n",
    "    \n",
    "    # 准确率\n",
    "    correct_prediction = tf.equal(tf.argmax(y_,axis=1),tf.argmax(y,axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    # 全局步数\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # 滑动平均操作\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVETAGE_DECAY, global_step)\n",
    "    # 滑动平均操作作用到各个可训练变量中\n",
    "    variables_averages_op = variable_averages.apply(\n",
    "        tf.trainable_variables())\n",
    "    # 交叉熵\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_)\n",
    "    # 交叉熵平均化\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    # 损失函数\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    # 学习率设置\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate=LEARNING_RATE_BASE,\n",
    "        decay_rate=LEARNING_RATE_DECAY,\n",
    "        global_step=global_step,\n",
    "        decay_steps=mnist.train.num_examples/BATCH_SIZE)\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    # 初始化TensorFlow持久化类\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        # 只有训练，验证和测试在一个独立的程序完成\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step],feed_dict={x:xs, y_:ys})\n",
    "            if i % 1000 == 0:\n",
    "                print('After {} training step(s), loss on training batch is {}'.format(step, loss_value))\n",
    "                \n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 定义执行过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    train(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-f6b46463ced9>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-3-c6b79331efcd>:26: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "After 1 training step(s), loss on training batch is 3.018348455429077\n",
      "After 1001 training step(s), loss on training batch is 0.31889426708221436\n",
      "After 2001 training step(s), loss on training batch is 0.1641382873058319\n",
      "After 3001 training step(s), loss on training batch is 0.19242046773433685\n",
      "After 4001 training step(s), loss on training batch is 0.11652451753616333\n",
      "After 5001 training step(s), loss on training batch is 0.10970417410135269\n",
      "After 6001 training step(s), loss on training batch is 0.09458530694246292\n",
      "After 7001 training step(s), loss on training batch is 0.08456099033355713\n",
      "After 8001 training step(s), loss on training batch is 0.08123067021369934\n",
      "After 9001 training step(s), loss on training batch is 0.07128529995679855\n",
      "After 10001 training step(s), loss on training batch is 0.06576739996671677\n",
      "After 11001 training step(s), loss on training batch is 0.06526441872119904\n",
      "After 12001 training step(s), loss on training batch is 0.05757316201925278\n",
      "After 13001 training step(s), loss on training batch is 0.05329569801688194\n",
      "After 14001 training step(s), loss on training batch is 0.05110922083258629\n",
      "After 15001 training step(s), loss on training batch is 0.05226686969399452\n",
      "After 16001 training step(s), loss on training batch is 0.04576839879155159\n",
      "After 17001 training step(s), loss on training batch is 0.049508798867464066\n",
      "After 18001 training step(s), loss on training batch is 0.042365361005067825\n",
      "After 19001 training step(s), loss on training batch is 0.0419442318379879\n",
      "After 20001 training step(s), loss on training batch is 0.04110534116625786\n",
      "After 21001 training step(s), loss on training batch is 0.04611041396856308\n",
      "After 22001 training step(s), loss on training batch is 0.03973083198070526\n",
      "After 23001 training step(s), loss on training batch is 0.04009482264518738\n",
      "After 24001 training step(s), loss on training batch is 0.036511149257421494\n",
      "After 25001 training step(s), loss on training batch is 0.037885621190071106\n",
      "After 26001 training step(s), loss on training batch is 0.03423982858657837\n",
      "After 27001 training step(s), loss on training batch is 0.03854171931743622\n",
      "After 28001 training step(s), loss on training batch is 0.0382780022919178\n",
      "After 29001 training step(s), loss on training batch is 0.035195477306842804\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3299: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
